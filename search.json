[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course overview",
    "section": "",
    "text": "Statistical Methods for Archaeological Data Analysis (SMADA)\nSpring semester, 2023 (FS2023)\nStatistics has become an indispensable tool in prehistoric archaeology. This course is intended on the one hand to give the participants the skills to understand and reproduce statistical analyses in literature and on the other hand to enable them to use such analyses for their scientific work themselves.\nBasic statistical concepts will be explained and simple uni- and bivariate methods of descriptive, explorative and inductive statistics will be presented. An important part is the practical application of these methods. This is to be carried out with the help of the statistics software R, a open source and free-of-charge, yet extremely powerful computing environment.\nThe course will be taught in English. It is suitable for students of all semesters; no statistical knowledge or expertise in special computer programs is required."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Course overview",
    "section": "Learning objectives",
    "text": "Learning objectives\nThe aim of this course is to provide students with the skills to understand and reproduce statistical analyses of archaeological data. By the end of this course, they should:\n\nBe familiar with the fundamental concepts of exporatory data analysis (EDA), frequentist hypothesis-testing, regression modelling, and multivariate analysis\nBe proficient enough in R programming to reproduce a full data science workflow, including data import, transformation, and analysis\nBe able to independently locate learning resources and troubleshoot problems in R code in order to extend the range of statistical procedures they can perform in future"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Course overview",
    "section": "Instructors",
    "text": "Instructors\n Joe Roe &lt;joseph.roe@iaw.unibe.ch&gt;"
  },
  {
    "objectID": "index.html#learning-format",
    "href": "index.html#learning-format",
    "title": "Course overview",
    "section": "Learning format",
    "text": "Learning format\nThe primary format of instruction will be practical tutorials, where students will be given a foundation in new concepts in self-guided, computer-based exercises, and consolidate their skills through extended problem sets. During contact hours the instructor will be on hand to problem solve and explain more complex concepts; open discussion is welcome and encouraged! It is expected that these tutorials will take longer than the time allotted for the class to complete, i.e. they are also homework.\nRelevent background material will be introduced in readings, pre-recorded videos and other resources, which students are expected to have studied before each weekly tutorial."
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Course overview",
    "section": "Evaluation",
    "text": "Evaluation\nStudents are asked to maintain a ‘knowledge base’ (Zettelkasten) of notes on the key concepts they learn from the background material and tutorials as the main output of the course and the basis for evaluation. The exact format of the Zettelkasten is up to the individual student. More details will be provided over the course of the semester."
  },
  {
    "objectID": "tutorials/introduction.html",
    "href": "tutorials/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In this tutorial we will discuss the role of statistics in archaeology, review some recent trends and current issues in archaeostatistics, and introduce the learning aims, format, and practicalities for the rest of the course."
  },
  {
    "objectID": "tutorials/introduction.html#objectives",
    "href": "tutorials/introduction.html#objectives",
    "title": "Introduction",
    "section": "Objectives",
    "text": "Objectives\nBy the end of this tutorial you should:\n\nUnderstand the theoretical context and ‘uses of abuses’ of statistics in archaeology\nBe familiar with current issues and trends in archaeostatistics\nCritically engage with a recent application of statistics in archaeology"
  },
  {
    "objectID": "tutorials/introduction.html#prerequisites",
    "href": "tutorials/introduction.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\nNone."
  },
  {
    "objectID": "tutorials/introduction.html#mini-practical-moralizing-gods",
    "href": "tutorials/introduction.html#mini-practical-moralizing-gods",
    "title": "Introduction",
    "section": "Mini-practical: ‘Moralizing Gods’",
    "text": "Mini-practical: ‘Moralizing Gods’\n\nmoralizing_gods.csv – data from Whitehouse et al. 2019"
  },
  {
    "objectID": "tutorials/workflows.html",
    "href": "tutorials/workflows.html",
    "title": "Data analysis workflows",
    "section": "",
    "text": "This tutorial covers how to organise a data analysis workflow in R, including code style, managing the environment, producing literate programming documents, and structuring projects as self-contained and reproducible research compendiums of data and code."
  },
  {
    "objectID": "tutorials/workflows.html#prerequisites",
    "href": "tutorials/workflows.html#prerequisites",
    "title": "Data analysis workflows",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nMarwick (2016), Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation, Journal of Archaeological Method and Theory 24: 424–450."
  },
  {
    "objectID": "tutorials/eda_with_r.html",
    "href": "tutorials/eda_with_r.html",
    "title": "Exploring data with R",
    "section": "",
    "text": "In this tutorial we will look deeper at the R programming language and how it can be used for exploratory data analysis (EDA).\nYou should run all the R code examples given yourself and answer the exercises at the end of each section. Remember to also make notes on key concepts as you encounter them."
  },
  {
    "objectID": "tutorials/eda_with_r.html#objectives",
    "href": "tutorials/eda_with_r.html#objectives",
    "title": "Exploring data with R",
    "section": "Objectives",
    "text": "Objectives\nBy the end of this tutorial, you should:\n\nRecognise the building blocks of the R environment: objects, values, and functions.\nUse the R console to load a dataset, calculate a statistic and produce a plot\nBegin your Zettelkästen with notes on the key pieces of R syntax you have learned"
  },
  {
    "objectID": "tutorials/eda_with_r.html#prerequisites",
    "href": "tutorials/eda_with_r.html#prerequisites",
    "title": "Exploring data with R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAndy Matuschak, ‘Evergreen notes’\nInstall R on your computer"
  },
  {
    "objectID": "tutorials/eda_with_r.html#getting-started-with-r",
    "href": "tutorials/eda_with_r.html#getting-started-with-r",
    "title": "Exploring data with R",
    "section": "Getting started with R",
    "text": "Getting started with R\nR is a ‘statistical computing environment’. It combines a programming language (R) with an interactive console for exploring and visualising data, as well as thousands of extensions (packages) that can be installed to perform specialised tasks.\nOn Windows and macOS, the usual way to install R is to download a binary installer from CRAN. For Linux, it can be found in the official repositories of most distributions. Windows users should make sure to install the Rtools package too, which is available from the same page. Once installed, you should be able to launch RGui, which gives you access to the R console and a collection of other tools (e.g. for installing add-on packages). Alternatively, you can launch the R console directly from your operating system’s terminal.\n\n\n\n\n\n\nR IDEs\n\n\n\nAs you become more proficient in R, you may want to switch to a more complex ‘integrated development environment’ such as RStudio or VSCode, which provide a range of advanced tools for editing and running R code. However I recommend sticking with the basic R console to begin with, to remove distractions and help you focus on how the R environment changes as you input commands."
  },
  {
    "objectID": "tutorials/eda_with_r.html#the-r-environment",
    "href": "tutorials/eda_with_r.html#the-r-environment",
    "title": "Exploring data with R",
    "section": "The R environment",
    "text": "The R environment\nR’s engine is its console, the command-line interface through which you interact with the R environment. When you first start the console, you will be presented with a welcome message like this:\nR version 4.2.2 (2022-10-31) -- \"Innocent and Trusting\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; \nThe &gt; character at the bottom is R’s command prompt. It indicates that R is waiting for you to tell it to do something! Try typing in a simple arithmetic expression such as 2 + 2:\n\n2 + 2\n\nR responds with the answer: 4 (we will get back to what the [1] means later). The numbers in this expression represent the first building block of the R environment: values. These simply represent what they are, e.g. the value 2 is the number 2.\n\nValues\nIf we enter a value into the prompt on its own, R simply prints it back for us:\n\n2\n\nValues don’t have to be numbers. Text can be entered as a string or character value, which is represented by text surrounded by \"double quotes\":\n\n\"Hello world!\"\n\nAgain, presented with a value alone, R simply prints it. We can’t do arithmetic with string values (that wouldn’t make sense), but we can manipulate them in other ways:\n\ntoupper(\"Hello world!\")\n\nIn total, there are six basic data types that can be represented as values in R:\n\n\n\n\n\n\n\n\nType\nExamples\nDescription\n\n\n\n\ndouble\n1.2\nA decimal number, known for historical reasons as a double precision value.\n\n\ninteger\n1L\nA whole number; an integer.\n\n\ncharacter\n\"abc\"\nText, also known as a string. Character values can also be entered with single quotes, e.g. 'abc', but double quotes is the norm.\n\n\nlogical\nTRUE, FALSE\nA binary or boolean value, representing ‘yes’ or ‘no’, ‘true’ or ‘false’, etc. Logical values can also be entered with the abbreviations T and F, but this should be avoided because it is possible to do T &lt;- FALSE and wreak havoc.\n\n\ncomplex\n1i\nA complex number\n\n\nraw\nas.raw(255)\nThe raw bytes underlying other values\n\n\n\nIn most cases R converts freely between doubles and integers, so they are often grouped together and called numerics. Complex and raw values are rarely encountered in practice and are included here only for completeness.\nLogical values give us the possibility of using R to evaluate logical statements. For example, we can test if two values are equal to one another using the == operator:\n\n2 == 2\n1 == 2\n\"Hello\" == \"HELLO\"\n\nThe result is a logical value: TRUE or FALSE. We will look at logical operations in more detail in the next section. For now, be aware that R sometimes try to be clever and interpret one data type as another:\n\n3L == 3\n3 == \"3\"\nTRUE == 1\n\nBut it is not that clever:\n\n3 == \"three\"\n\"false\" == FALSE\n\n\nVectors\nR doesn’t limit us to single values. We can combine them together into a vector by entering a list of values separated with a comma and enclosed in c(    ). For example, a sequence of numbers:\n\nc(1, 2, 3, 4, 5)\n\nOr a series of strings:\n\nc(\"red\", \"orange\", \"yellow\", \"green\", \"blue\")\n\nThere is also a shortcut syntax for creating vectors with a sequence of consecutive numbers:\n\n1:10\n\nVectors enable vector operations. For example, if we do arithmetic with two vectors, R will repeat the operation on each pair of values (note that this is not what a mathematician would do if asked to multiply vectors):\n\nc(1, 2, 3) * c(3, 2, 1)\nc(\"A\", \"B\", \"C\") == c(\"A\", \"b\", \"c\")\n\nIn fact, R considers all values to be vectors: a value like 2 is simply a vector of one number. This is why when we print even a single value it is proceeded by a [1] (indicating the initial position in a vector). Vector operations are fundamental to R and by default anything that you can do with a single value, you can also do with a vector:\n\nc(1, 2, 3) * 2\ntoupper(c(\"red\", \"orange\", \"yellow\", \"green\", \"blue\"))\n\nNote that vectors can only contain one data type. If you try to combine more than one data type, R will try to find the ‘common denominator’:\n\nc(1, 2, \"3\")\n\n\n\nNA and null values\nIn addition to the six basic data types, R recognises a number of special values that represent the absence of a value.\nThe most common of these is NA, which in R means “missing data” or “unknown”. When importing tables of data into R, empty cells will be read as NA. NA values can occur in any of the six basic data types, for example:\n\nc(1, NA, 3)\nc(\"red\", NA, \"yellow\")\n\nImportantly, NA is not equivalent to 0 or FALSE; those are known values. Generally speaking, introducing NA into an expression ‘propagates’ the unknown value:\n\nNA == 5\nNA * 5\n\nThis is because R doesn’t know if NA is 5. It could be; it could not. It also doesn’t know what the result of multiplying an unknown value by 5 is.\nThe strictness with which R treats NAs can be frustrating at times, but it is mathematically well-founded and generally protects you from doing silly things (as we saw last week!)\n\n\nExercises\n\nUse R to calculate the mean of these ten random numbers: 69, 37, 89, 22, 96, 74, 44, 7, 20, 45\nWhat is the result of c(FALSE, TRUE, 2, 3) == 0:3? Why?\nWhat is the result of NA == NA? Why?\n\n\n\n\nObjects\nApart from values, R interprets everything you type into the console as the name of an object. This is why we have to surround character values with quotes, to distinguish them from objects:\n\n\"version\"\nversion\n\nWhy do these two commands produce such different output? \"version\" is a value – the word ‘version’. version is an object, storing information on the version of R you have installed.\n\nErrors\nMost words aren’t the name of an object, so if you forget to enclose a character value with quotes (which is easily done), you are likely to encounter an error like this:\n\nhello\n\n“Error: object ‘hello’ not found” tells us fairly straightforwardly what the problem is: there is no object called hello.\nThis is the first of many errors you are going to see on this course! What if, for example, we try to multiply two strings, an operation that doesn’t make sense?\n\n\"Hello world!\" * \"Goodbye, world...\"\n\nThis time the message (“non-numeric argument to binary operator”) is pretty cryptic, which is unfortunately quite common with R. Essentially what it means is that R does not know how to add (a “binary operation”) something that is not a number (a “non-numeric argument”). But however cryptic they are, it’s important that you pay attention to errors when they occur. R never raises errors without reason and it is unlikely that you will be able to continue with your work until you have resolved them.\n\n\n\n\n\n\nDeciphering R error messages\n\n\n\nAs you get used to the jargon used in R’s error and warning messages, you will find that they do actually explain what the problem is. In the meantime, apart from asking your instructor, you can always try just pasting the error into a search engine. Unless you’re doing something truly exotic, it is very likely that someone else has encountered it before and has explained what it means in ordinary language.\nWhen you run more than one line of R code at a time, an error in one line is likely to produce errors in subsequent code. In these circumstances, it’s important to start investigating the problem with the first error encountered. If you encountered an error when trying to create an object, for example, you might subsequently get the “object not found” error when trying to use it – but this doesn’t tell you the root of the problem.\n\n\n\n\nAssignment\nThe R environment is initialised with a certain number of in-built objects, like version. Much more useful, however, is creating objects yourself. For this, we use the assignment operator, &lt;-:\n\nx &lt;- 2\nx\n\nWhat is happening here? In the first line, we assign the value 2 to the object x. In the second line, we ask R to print the object x, which is 2.\nAssignment is how we can start to build up routines that are more complex than those we could do on a calculator. For example, we can break down the exercise of calculating the mean into several steps:\n\ntotal &lt;- 10 + 20 + 30 + 40 + 50\nn &lt;- 5\ntotal / n\n\nAs you populate the R environment with objects, it can be easy to get lost. For example, what if I forget that I’ve already used the name n and reassign it to something else? Then if I try to calculate the mean again:\n\nn &lt;- \"N\"\ntotal / n\n\nEntering ls() lists the objects currently in the R environment, so we can keep track:\n\nls()\n\nIf you need to clean things up, you can remove objects with rm():\n\nrm(n)\nls()\n\nOr to start completely afresh, simply restart R: objects are not saved between sessions.\n\n\n\n\n\n\nNaming objects\n\n\n\nAs the work you do in R becomes more complicated, choosing good names for objects is crucial for keeping track. There are some hard rules about object names:\n\nObject names can’t contain spaces\nObject names can’t start with a number\nObject names can only contain letters, numbers, _ and .\nObject names are case sensitive (e.g. X is different from x)\n\nAnd some good tips:\n\nPick a convention for separating words (e.g. snake_case or camelCase) and stick to it\nIt is better to use a long and descriptive name (e.g. switzerland_sites) than a short, abstract one (e.g. data)\nDon’t reassign object names unless you have a good reason to\n\n\n\n\n\nExercises\n\nCreate objects containing a) a vector of numbers, b) a single character value and c) the sum of a set of numbers\nWhy does 3sites &lt;- c(\"Stonehenge\", \"Giza\", \"Knossos\") produce an error?\nWhat would you call an object containing the mean site size of Neolithic sites in Germany?\n\n\n\n\nFunctions\nFunctions are a special type of object that do something with other objects or values (called arguments). You have already met several functions in this tutorial: arithmetic operators like +, logical operators like ==, and regular functions like ls().\nThese show two different function syntaxes. The arithmetic and logical operators use infix syntax, with two arguments and the function name (the operator) in the middle:\n\n2 + 2\n\nMost functions, and any that can have more than two arguments, use regular function syntax. This looks something like this:\nfunction(argument1, argument2, argument3)\nls() is a function of this style, which takes no arguments (hence the empty brackets). An example of a function that does take arguments is sum():\n\nsum(10, 20, 30, 40, 50)\n\nFunction arguments can also be objects:\n\nx &lt;- 1:100\nsum(x)\n\n\nExercises\n\nCalculate the mean of these ten random numbers using the sum() function: 59, 51, 20, 70, 86, 50, 100, 92, 83, 80\nWhy is it a bad idea to assign the mean of a variable to a new object called mean?"
  },
  {
    "objectID": "tutorials/eda_with_r.html#exploratory-data-analysis-with-r",
    "href": "tutorials/eda_with_r.html#exploratory-data-analysis-with-r",
    "title": "Exploring data with R",
    "section": "Exploratory data analysis with R",
    "text": "Exploratory data analysis with R\nNow you’re familiar with the fundamental building blocks of the R environment, we’re ready to put them into action in exploring some data. For this, follow sections 2.1 and 2.2 in R for Data Science (2nd edition), including the exercises."
  },
  {
    "objectID": "tutorials/eda_with_r.html#extension-exercises",
    "href": "tutorials/eda_with_r.html#extension-exercises",
    "title": "Exploring data with R",
    "section": "Extension exercises",
    "text": "Extension exercises\n\nReview the highlighted concepts in this tutorial and make sure that your notes cover them.\nIdentify and fix the errors in the following code:\n\n\nlibary(todyverse)\n\nggplot(dTA = mpg) + \n  geom_point(maping = aes(x = displ y = hwy)) +\n  geom_smooth(method = \"lm)"
  },
  {
    "objectID": "tutorials/tidy_data.html",
    "href": "tutorials/tidy_data.html",
    "title": "Importing and tidying data",
    "section": "",
    "text": "So far, the data we have used in the course has come prepared in packages. In practice, we are much more likely to work with data from other sources (published datasets, data entered in a spreadsheet, data from APIs, etc.) that need to be imported in R. This tutorial introduces tools for importing data in various formats into R and—since real-world data rarely comes to us perfectly well-formatted—how to transform data with dplyr."
  },
  {
    "objectID": "tutorials/tidy_data.html#objectives",
    "href": "tutorials/tidy_data.html#objectives",
    "title": "Importing and tidying data",
    "section": "Objectives",
    "text": "Objectives\nThis tutorial should enable you to:\n\nImport data in delimited text (e.g. CSV) and spreadsheet (e.g. XSLX) files into R \nUse dplyr to filter, mutate, group, and summarise imported data"
  },
  {
    "objectID": "tutorials/tidy_data.html#prerequisites",
    "href": "tutorials/tidy_data.html#prerequisites",
    "title": "Importing and tidying data",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBroman & Woo (2017), Data Organization in Spreadsheets, The American Statistician 72(1): 2–10."
  },
  {
    "objectID": "tutorials/tidy_data.html#importing-data",
    "href": "tutorials/tidy_data.html#importing-data",
    "title": "Importing and tidying data",
    "section": "Importing data",
    "text": "Importing data\nWhatever the format, importing data into R typically involves three steps:\n\nRead the original file into R with an appropriate function\nResolve any problems that prevent R from parsing the data correctly (e.g. remove special characters from column names, skip malformed rows)\nAssign the imported data to an R object with an appropriate type (e.g. a data frame)\n\n\nDelimited text\nDelimited text is a very widely used format for storing tabular data as plain text, where each row has its own line and the columns are separated by a delimiter. The most common variant is comma-separated values (CSV, .csv), followed by tab-separated values (TSV, .tsv). Microsoft Office software often exports files with semicolon or space-separated values using the generic file extension .txt.\nThere are number of functions for reading delimited text files included in base R (e.g. read.csv(), read.delim()). However, I recommend using the package readr instead, because it has a more consistent syntax and can resolve a number of common parsing problems automatically.\nWe can read a standard CSV file with read_csv(), without any additional arguments. For example, we can read nerd.csv, from the NERD database of radiocarbon dates from the Near East:\n\nlibrary(readr)\nread_csv(\"../data/nerd.csv\")\n\nThe first argument is the path to the file, relative to R’s working directory. You will need to adjust this to reflect the location where you saved nerd.csv.\n\n\n\n\n\n\nFile paths in R\n\n\n\nR uses Unix-style file paths to locate files. Directories are separated by a forward slash (/), e.g. /home/user/path/to/file.txt – even on Windows, which usually uses a backslash (\\). The name of the file must always be given in full, i.e. with its file extension (.txt in the previous example). If the path starts with a slash (or on Windows, a drive label like C:), it is an absolute path. Absolute paths will work to locate a file anywhere on your computer, regardless of how R was started, but will almost certainly not work on other computers.\nIt’s usually better to use a relative path, especially if you intend to share your script with others. These locate files relative to R’s working directory, which in turn depends on how R was launched. You can find what your current working directory is with getwd(). If our working directory contained a directory named data with in a file in it called file.csv, we could reference that with the relative path data/file.csv.\nMost readr functions also allow you to replace a path with a URL, in which case it downloads the file to a temporary directory before reading it.\n\n\nNotice the message about column types. readr will try to guess what type of vector each column holds – usually correctly. But if you need to correct something, or don’t want to see the message, we can specify column types manually using the col_types argument (see ?read_csv for instructions).\nThere are similar functions for other standardised types, for example tab-separated values like nerd.tsv:\n\nread_tsv(\"../data/nerd.tsv\")\n\nOr we can use read_delim() for generic delimited text. For example, ‘European-style’ CSVs commonly use a comma (,) as a decimal separator, and so to avoid confusion use a semicolon (;) as a delimiter. nerd.txt is an example of this format, which we can read by adjusting the delim and locale arguments:\n\nread_delim(\"../data/nerd.txt\", delim = \";\", locale = locale(decimal_mark = \",\"))\n\nAlthough for this specific variant, there is actually a shortcut in read_csv2():\n\nread_csv2(\"../data/nerd.txt\")\n\nNote that the file extension (.txt) doesn’t have to match the function used.\nThe result of all these read_* functions is a data frame (or a variant of it called a tibble), which we can assign to an R object as usual:\n\nnerd &lt;- read_csv2(\"../data/nerd.txt\")\n\n\n\nSpreadsheets\nreadxl provides functions for reading spreadsheets in Excel formats (.xls, .xlsx) with a readr-style syntax. For example, 14cpalaeolithic.xlsx contains another radiocarbon database in Excel format:\n\nlibrary(readxl)\nread_xlsx(\"../data/14cpalaeolithic.xlsx\")\n\nA useful feature of this function is that we can control the sheet and range (using Excel-style syntax) we want to use. In this case, the radiocarbon data in 14c-palaeolithic is only a subset of the first sheet:\n\nread_xlsx(\"../data/14cpalaeolithic.xlsx\", sheet = \"Blad1\", range = \"A1:S35086\")\n\nIn this way we can avoid importing a lot of data we won’t use.\n\n\n\nExercises\n\nImport neonet.tsv (from https://doi.org/10.5334/joad.87) into R\nWhat other options can we control with locale()?\nImport flohr_et_al.xlsx (from https://doi.org/10.1016/j.quascirev.2015.06.022) into R\nTry passing the argument .name_repair = \"universal\" when importing flohr_et_al.xlsx: what changes? Why is this useful?"
  },
  {
    "objectID": "tutorials/tidy_data.html#data-transformation",
    "href": "tutorials/tidy_data.html#data-transformation",
    "title": "Importing and tidying data",
    "section": "Data transformation",
    "text": "Data transformation\n\nWork through Chapter 5 – Data Transformation in R for Data Science (1st ed.), including all code examples and exercises.\n\nExercises\nUsing dplyr and the islay_lithics datset:\n\nAdd a column to the data frame calculating the total number of lithics\nWhat are the top 3 sites by total number of lithics?\nHow many sites have 10 or more total lithics?\nOf these sites, calculate the average number of lithics per site for each period"
  },
  {
    "objectID": "tutorials/testing.html",
    "href": "tutorials/testing.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "This tutorial introduces the most common methods in the null-hypothesis significance testing (NHST) framework, a highly influential frequentist approach to statistics developed in the 20th century. The basic idea of this approach is to use probability theory to test whether an observed pattern can be considered statistically significant. More formally, it tries to quantify how confident we can be about rejecting the null hypothesis - that any pattern or relationship we think we see is just down to chance. In practice, this amounts to choosing a test statistic that is appropriate to the sample you have available and comparing it to a reference distribution to calculate the probability of observing that statistic under the null hypothesis (the ‘p-value’)."
  },
  {
    "objectID": "tutorials/testing.html#prerequisites",
    "href": "tutorials/testing.html#prerequisites",
    "title": "Hypothesis testing",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nChapter 5 (“An Introduction to Statistical Inference”), in Stephen Shennan (1998), Quantifying Archaeology, 2nd edition. https://doi.org/10.1515/9781474472555-005"
  },
  {
    "objectID": "tutorials/testing.html#objectives",
    "href": "tutorials/testing.html#objectives",
    "title": "Hypothesis testing",
    "section": "Objectives",
    "text": "Objectives\nThis tutorial should enable you to:\n\nUnderstand the key concepts behind the NHST framework\nRecognise a normal distribution and know how to deal with non-normal data\nSelect an appropriate statistic to test for significant differences in a) numeric and b) categorical distributions and compute it in R"
  },
  {
    "objectID": "tutorials/testing.html#is-my-data-normal",
    "href": "tutorials/testing.html#is-my-data-normal",
    "title": "Hypothesis testing",
    "section": "Is my data normal?",
    "text": "Is my data normal?\nMost NHST tests are parametric, meaning that they make assumptions about the way the data was collected. These assumptions essentially describe data collected in randomised experiments (which, not surprisingly, is where NHST originated): that we have a sample of the population we’re interested in; that this sample was drawn randomly, each observation independent of the others; and that the resulting variable has a normal distribution. The first two assumptions are out of our control as data analysts but the third—the normality assumption—is something we can investigate and, possibly, correct.\nHow can we tell if data is normally distributed? The most obvious way, which is frequently all you need, is to inspect it graphically using the techniques we learned for visualising distributions. Plotting the distribution of bill length of Adelie penguins from the palmerpenguins dataset, for example: \n\nlibrary(ggplot2)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(palmerpenguins)\n\nadelie &lt;- filter(penguins, species == \"Adelie\")\nggplot(adelie, aes(bill_length_mm)) + geom_density()\n\n\nWe can see that the distribution definitely looks normal. If we wanted to be really sure (or convince someone else!) that it was normal enough to work with in NHST, we can use the Shapiro-Wilk test of normality:\n\nshapiro.test(adelie$bill_length_mm)\n\nThis reports the test statistic (W) and an associated p-value. The null hypothesis of the Shapiro-Wilk test is that the variable is normally distributed; here we see a high p-value, meaning that we cannot be confident about rejecting this null hypothesis. In other words, it indicates that the distribution is probably drawn from a normal distribution.\nUnfortunately, as we have already found out, it is quite common to see markedly not-normal distributions in archaeological data. Take the number of lithics found at sites on Islay, for example:\n\nlibrary(islay)\n\nggplot(islay_sites, aes(total_chipped_stone)) + geom_density()\n\nIn case we weren’t already certain, Shapiro-Wilk tells us we can very confidently reject the null hypothesis of normality:\n\nshapiro.test(islay_sites$total_chipped_stone)\n\nGiven a non-normal distribution like this, we have two options. The first is to use one of a number of nonparametric tests—NHST techniques that do not make assumptions about the distribution of the data—which we will look at shortly.\nOr, we can try to make the data normal. This option is particularly attractive here because the distribution of chipped stone counts resembles a log-normal distribution, which can easily be transformed into a normal distribution by taking its logarithm:\n\nislay_sites &lt;- mutate(islay_sites, log_chipped_stone = log(total_chipped_stone))\n\nggplot(islay_sites, aes(log_chipped_stone)) + geom_density()\nshapiro.test(islay_sites$log_chipped_stone)\n\nThus we could precede with parametric NHST using log_chipped_stone instead of total_chipped_stone.\n\n\n\n\n\n\nA question of power\n\n\n\nYou might wonder, if parametric tests are so fussy about what kind of data they’ll accept, but nonparametric tests will take anything, why don’t we just always use nonparametric ones? One reason for this is historical: parametric tests make the assumptions to do because they make it much easier to calculation the test statistic and its p-value. In the pre-computer era, ease of calculation was extremely important, and this parametric testing became the norm. Another reason—now more relevant—is that these assumptions tend to give parametric more statistical power: they’re better at detecting small but significant effects, while avoiding false positives.\n\n\n\nExercises\n\nIs the body mass of Adelie penguins normally distributed?\nislay_land contains the area of Islay itself and the islets around it. Describe its distribution.\nTransform area such that it could be used in a nonparametric test."
  },
  {
    "objectID": "tutorials/testing.html#testing-the-difference-between-numeric-distributions",
    "href": "tutorials/testing.html#testing-the-difference-between-numeric-distributions",
    "title": "Hypothesis testing",
    "section": "Testing the difference between numeric distributions",
    "text": "Testing the difference between numeric distributions\nHaving established whether our data is normal (or not), we can move on to testing hypotheses about it. A very common class of question is whether there is a statistically significant difference between two samples. Or, put another way, whether two sample were drawn from the same population. You can reduce a lot of types of archaeological questions to this type of hypothesis:\n\nAre two groups of artefact different enough to be called “types”?\nDo sites in different periods or contexts differ in a certain characteristic?\nReversing the question, is an assemblage from one site likely to be from the same ‘population’ as an assemblage in another?\n\nWe’ll look at whether the number of chipped stone artefacts differs between Mesolithic and Later Prehistoric sites. We can start, as always, by inspecting the data graphically:\n\nfilter(islay_sites, period %in% c(\"Mesolithic\", \"Later Prehistoric\")) |&gt;\n    ggplot(aes(total_chipped_stone)) + \n        facet_wrap(~period, ncol = 1, scales = \"free_y\") + \n        geom_density()\n\nThey look a little different, but it’s hard to know if this is just because of unequal sampling. This is where NHST can be handy.\nStudent’s t test is a parametric test for the equality of the means of two distributions. If we inspect the documentation for its R function, ?t.test, we can see that we need to tell it whether the variance of the distributions is equal or not. To decide this, we can turn to another test, the F test for equal variance:\n\nmesolithic &lt;- filter(islay_sites, period == \"Mesolithic\")\nlater &lt;- filter(islay_sites, period == \"Later Prehistoric\")\nvar.test(mesolithic$log_chipped_stone, later$log_chipped_stone)\n\nRemember, we’ve found that the total chipped stone is not normally distributed, so to do these parametric tests we use log_chipped_stone instead. This tells us that the variances are probably equal, so we can precede with the t test using var.equal = FALSE:\n\nt.test(mesolithic$log_chipped_stone, later$log_chipped_stone)\n\nThough the p value is quite small, it is above 0.05, so cannot safely reject the null hypothesis. * Nonparametric: Mann-Whitney U * Nonparametric: Kolmogorov-Smirnov\n\nExercises\n\nIs there a significant difference between the number of chipped stones at Mesolithic and Later Prehistoric sites on Islay?\nGive a null hypothesis and alternative hypothesis that could help answer the following archaeological questions:\n\nA zooarchaeologist has measured the length of deer long bones at a hunter-gatherer site and an early farming site in the same region. She theorises that the two sites were hunting from different deer populations. Is this likely?\nYou have a newly-discovered site from which an assemblage of bronze arrowheads have been excavated. You also have a set of measurements of similar arrowheads from a Bronze Age culture in an adjacent region. Based on the arrowheads, does your new site belong to this culture?\nWere Neolithic people shorter than Mesolithic people?\n\nTwo non-parametric versions of the t-test are the Kolmogorov-Smirnov test and the Mann-Whitney U test. Perform each in R using the (untransformed) number of chipped stones from sites on Islay. Do the results differ from the t test above?"
  },
  {
    "objectID": "tutorials/summary_statistics.html",
    "href": "tutorials/summary_statistics.html",
    "title": "Summary statistics",
    "section": "",
    "text": "In this tutorial we will look at statistics used to describe or summarise characteristics of a variable such as its , central tendency (‘average’), dispersion, or distribution."
  },
  {
    "objectID": "tutorials/summary_statistics.html#prerequisites",
    "href": "tutorials/summary_statistics.html#prerequisites",
    "title": "Summary statistics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nChapter 4 (“Numerical Summaries of a Single Variable”), in Stephen Shennan (1998), Quantifying Archaeology, 2nd edition. https://doi.org/10.1515/9781474472555-005"
  },
  {
    "objectID": "tutorials/summary_statistics.html#objectives",
    "href": "tutorials/summary_statistics.html#objectives",
    "title": "Summary statistics",
    "section": "Objectives",
    "text": "Objectives\nThis tutorial should enable you to:\n\nDescribe the distribution of a variable in terms of central tendency, dispersion, and shape\nCalculate summary statistics to measure these characteristics"
  },
  {
    "objectID": "tutorials/summary_statistics.html#describing-a-distribution",
    "href": "tutorials/summary_statistics.html#describing-a-distribution",
    "title": "Summary statistics",
    "section": "Describing a distribution",
    "text": "Describing a distribution\nWe’ve looked at a lot of different distributions graphically; how would you describe them in words? We can break it down a few characteristics:\n\nCentral tendency – what is the midpoint of the distribution, the ‘typical’ value of the variable?\nDispersion – how widely does the variable vary around this central tendency? How ‘spread out’ is it?\nShape – is the distribution symmetrical around the central tendency, or skewed? If so, in what direction is it skewed?\nModality – is there actually a single central tendency? Or two? Or more?\n\nAs well as describing these characteristics in words, we can measure them numerically using a set of functions known collectively as summary statistics.\nAnother way of describing a distribution—a sort of shorthand—is to compare it to one of a number of mathematical probability distribution functions that are known to occur frequently in real-world data. The most prominent of these is the normal distribution:\nThe normal distribution is so named because it is what statisticians ‘normally’ expect to see when we measure a sample drawn randomly from a larger population. As such, many statistical tests in the classical hypotheses-testing framework assume that the variable or variables being tested are normally distributed – so it’s important to check that they are before running them! These tests are called parametric; tests that don’t assume normality are non-parametric.\nArguably more common in archaeostatistics is the log normal distribution:\nCompared to the normal distribution, a log normal distribution is skewed towards smaller values (it is asymmetric). Another important difference is that the values in a log normal distribution cannot be negative – much like many ‘real world’ values archaeologists measure. It is called log-normal because it is the logarithm of the normal distribution. This is handy because it means that, if you encounter a log normal distribution, you can easily transform it into a normal distribution and so use parametric tests.\nStatisticians have described many, many other distribution functions. Fitting empirical variables to theoretical distributions becomes important in statistical modelling, under both the classical hypothesis-testing and the Bayesian framework. For now, just be on the look out for normal and not-normal distributions, as this has a more immediate effect on exploratory statistics.\n\nExercises\n\nDescribe the following distributions:"
  },
  {
    "objectID": "tutorials/summary_statistics.html#measuring-the-central-tendency",
    "href": "tutorials/summary_statistics.html#measuring-the-central-tendency",
    "title": "Summary statistics",
    "section": "Measuring the central tendency",
    "text": "Measuring the central tendency\nThe central tendency or average of a variable is a measure of its ‘typical’ value or, in geometric terms, the midpoint of its distribution. The most usual measures of central tendency are the mean and the median.\nThe arithmetic mean of a variable is the sum of its values divided by the number of values.\nWe have already seen how to calculate the mean of a vector in R using the function sum():\n\nx &lt;- sample(1:1000, 100) # Sample 100 random numbers in the range 1:1000\nsum(x) / 100\n\n\n\n\n\n\n\nCode comments\n\n\n\nR considers anything that comes after a # character to be a comment and does not try to interpret it as code. You can write anything you like as a comment; programmers often use them to explain lines of code that are not immediately obvious, to document how functions or scripts are intended to be used, or to leave notes for themselves and otherwise. Since they have no effect, you don’t need to copy comments when you are running the examples given here.\n\n\nOr if we don’t know how many values there are in advance, we can use the function length() to count how many elements there are in the vector:\n\nsum(x) / length(x)\n\nAlternatively, we can just use the function mean(), which does the same thing:\n\nmean(x)\n\nIf x contains NA values, we need to make sure to set the na.rm argument of mean():\n\nx[1] &lt;- NA # Replace the first value of with NA\nmean(x)\nmean(x, na.rm = TRUE)\n\nThe same is true of all the other statistical summary functions we will use in this tutorial.\nThe median is the central value of a variable, if you put that variable in its numerical sequence. It also has a convenient function in R, median():\n\nx &lt;- sample(1:1000, 100)\nmedian(x)\n\nThe choice of median or mode is largely determined by the shape of the distribution. If it is symmetrical (like the normal distribution), the mean is usually the best choice as it gives the most precise measurement. But with skewed distributions (like the log normal distribution), the median tends to be closer to the central tendency. The median is also less sensitive to outliers than the mean.\nA good practice is to visualise. By plotting the distribution, mean and median, we can see which looks more representative. For example, for a normal distribution (red = mean, blue = median):\n\nlibrary(ggplot2)\ndf &lt;- data.frame(x = rnorm(1000))\nggplot(df, aes(x)) +\n    geom_density() +\n    geom_vline(aes(xintercept = mean(x)), colour = \"red\") +\n    geom_vline(aes(xintercept = median(x)), colour = \"blue\")\n\n\n\n\nAnd for a log-normal distribution:\n\nlibrary(ggplot2)\ndf &lt;- data.frame(x = rlnorm(1000))\nggplot(df, aes(x)) +\n    geom_density() +\n    geom_vline(aes(xintercept = mean(x)), colour = \"red\") +\n    geom_vline(aes(xintercept = median(x)), colour = \"blue\")\n\n\n\n\n\nExercises\n\nCalculate the mean and median number of flakes from sites on Islay (using islay::islay_lithics)\nWhich is more appropriate? Why?\nCalculate the median of x without using the median() function"
  },
  {
    "objectID": "tutorials/summary_statistics.html#measuring-dispersion",
    "href": "tutorials/summary_statistics.html#measuring-dispersion",
    "title": "Summary statistics",
    "section": "Measuring dispersion",
    "text": "Measuring dispersion\nDispersion is the degree to which values of a variable vary from the central tendency. The simplest measure of the dispersion of a variable is its range, i.e. its highest and lowest values:\n\nrange(x)\n\nA simple range is obviously extremely sensitive to outliers. To mitigate this, it is more common to consider the range between quantiles – subsets of the data that exclude the extreme values. We can use the quantile() function to calculate quantiles. By default, it returns the quartiles (quarter-subsets) of the data:\n\nquantile(x)\n\nBut this can be customised with the probs argument:\n\nquantile(x, probs = seq(from = 0, to = 1, by = 0.1)) # tenth quantiles\n\nThe interquartile range is particularly commonly used. To calculate this, we simply take the different between the upper (75%) and lower(25%) quantiles:\n\ndiff(quantile(x, probs = c(0.25, 0.75)))\n\n\n\n\n\n\n\nOutliers\n\n\n\nOutliers are anomalously large or small values, i.e. they lie on the far ends of its distribution. Though they occur ‘naturally’ in almost all real data, it is generally assumed that they are not representative of the phenomenon we’re trying to measure. Extreme outliers can cause problems for statistical analyses, though some statistics and tests are more ‘sensitive to outliers’ than others. There is no objective, statistical way to determine what is and isn’t an outlier. Identifying and (potentially) removing them is always a combination of exploratory analysis (spotting large or small values) and your own knowledge expertise of the data (spotting whether these values are ‘real’ or not).\n\n\nAlternatively, we can use the standard deviation to summarise the dispersion of a dataset with a single number. The equation for this is a bit more complicated (see Shennan 1998 for details), but it is easily calculated in R:\n\nsd(x)\n\nThe standard deviation is widely used because it is easy to calculate, robust to outliers, and expressed in terms of distance to the mean (so it can be used to compare distributions with different central tendencies). A useful rule of thumb for interpreting the standard deviation is the 68–95–99.7 rule. This states that, if the variable is normally distributed:\n\n\n68% of the data lies within one standard deviation of the mean\n\n\n95% of the data lies within one standard deviation of the mean\n\n\n99.7% of the data lies within one standard deviation of the mean\n\n\n\nExercises\n\nCalculate the 5% and 95% quantiles (percentiles) of lithic flakes from sites on Islay.\nCalculate the standard deviation of lithic flakes from sites on Islay.\nExplain what the standard deviation tells you about the data using the 68-95-99.7 rule."
  },
  {
    "objectID": "tutorials/summary_statistics.html#measuring-shape",
    "href": "tutorials/summary_statistics.html#measuring-shape",
    "title": "Summary statistics",
    "section": "Measuring shape",
    "text": "Measuring shape\nBase R doesn’t have a lot of functions for summarising shape, so we will need to install and use the moments package:\n\n# install.packages(\"moments\")\nlibrary(\"moments\")\n\nSkew describes whether or not a variable has a symmetrical distribution. A distribution ‘leaning’ towards the left on a graph is negatively skewed; to the right positively skewed.\nWe can measure skew using the skewness() function from moments:\n\nskewness(x)\n\nKurtosis is the degree to which the distribution of a variable is ‘stretched out’. A variable with positive kurtosis might be described as having a ‘high peak’ and a ‘long tail’; with negative kurtosis, a ‘flat top’ or hill-shape.\nThere are two functions for measuring kurtosis in the moments package:\n\nkurtosis(x) # Pearson's measure of kurtosis\ngeary(x) # Geary's measure of kurtosis\n\nA final important shape characteristic is multimodality. So far, we’ve only worked with unimodal variables – distributed around a single central tendency. But it is not uncommon to see bimodal variables, with two distinct peaks, or multimodal variables, with three or more. At this point most of the statistics we calculated above will not be meaningful, and you have to investigate various techniques for decomposing the data into multiple, unimodal variables.\n\nExercises\n\nCalculate and interpret the skew of lithic flakes from sites on Islay\nCalculate and interpret the kurtosis of lithic flakes from sites on Islay\nWhat are some examples of archaeological datasets the we would expect to be multimodal? Why are these challenging from a statistical point of view?"
  },
  {
    "objectID": "tutorials/visualising_distributions.html",
    "href": "tutorials/visualising_distributions.html",
    "title": "Visualising distributions",
    "section": "",
    "text": "This tutorial is the first of two covering visualisations as a tool for exploratory data analysis. It introduces the package ggplot2, which provides a flexible ‘grammar of graphics’ for visualising data in R.\nThis tutorial focuses on visualising the distribution of data using bar plots, histograms, and density plots."
  },
  {
    "objectID": "tutorials/visualising_distributions.html#prerequisites",
    "href": "tutorials/visualising_distributions.html#prerequisites",
    "title": "Visualising distributions",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHadley Wickham, The Role of Visualization in Exploratory Data Analysis (video)\n\nIf you haven’t already, install the ggplot2 and islay packages. To install islay you will first need to install devtools; Windows users will additionally need RTools."
  },
  {
    "objectID": "tutorials/visualising_distributions.html#objectives",
    "href": "tutorials/visualising_distributions.html#objectives",
    "title": "Visualising distributions",
    "section": "Objectives",
    "text": "Objectives\nBy the end of this tutorial you should:\n\nBe able to generate questions about the distribution of a variable\nUnderstand the structure of the ‘grammar of graphics’\nKnow how to produce bar plots, box plots, histograms, and density plots in R using ggplot2"
  },
  {
    "objectID": "tutorials/visualising_distributions.html#exploring-data-generating-questions",
    "href": "tutorials/visualising_distributions.html#exploring-data-generating-questions",
    "title": "Visualising distributions",
    "section": "Exploring data – generating questions",
    "text": "Exploring data – generating questions\nEDA is fundamentally a process of asking questions of data. By generating a large number of questions, which explore different facets of different variables in our dataset, we hope to narrow down on the subset of questions that are interesting or reveal interesting patterns. At this point we don’t mean archaeological questions, but statistical ones:\n\nWhat variables are in my dataset?\nWhat type of variable are they?\nWhat variation is there in individual variables (what is their distribution)?\nWhat variation is there between individual variables (what is their relationship)?\n\nToday we will begin exploring the first three types of questions with the islay_sites dataset from the islay package. You can load it into R like so:\n\nlibrary(islay)\ndata(islay_sites)\n\nThis is a data frame, an import type of object in R that collects multiple vectors (of different types) into a table.\n\nExercises\n\nHow many sites are there in the islay_sites dataset?\nWhat information is recorded about sites in the islay_sites dataset?"
  },
  {
    "objectID": "tutorials/visualising_distributions.html#types-of-variable",
    "href": "tutorials/visualising_distributions.html#types-of-variable",
    "title": "Visualising distributions",
    "section": "Types of variable",
    "text": "Types of variable\nIn statistics, we break down variables (measurements of a particular quality of a thing) into two basic types: categorical variables are those that can be one of a fixed set of choices (like the colour of an object); numerical variables can take any of a range of values that can be interpreted as numbers (like the length of an object).\nThese are further divided into two sub-types each. Categorical variables can be nominal, in which the values are arbitrary (like colours), or ordinal, where they can be arranged in a sequence (like opinion, disagree – agree – strongly agree, etc.). Numerical variables can be discrete (whole numbers, typically from counts) or continuous (real numbers, typically from measurements).\n\nExercises\n\nGive an archaeological example of a nominal categorical variable, an ordinal categorical variable, a continuous numerical variable, a discrete numerical variable.\nWhich of the six data types could be used to represent each of these statistical types in R?\nWhat type of variable is each column of islay_sites?"
  },
  {
    "objectID": "tutorials/visualising_distributions.html#the-grammar-of-graphics",
    "href": "tutorials/visualising_distributions.html#the-grammar-of-graphics",
    "title": "Visualising distributions",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nIf you haven’t already, follow sections 2.1 and 2.2 in R for Data Science (2nd edition), for an introduction to plotting with ggplot2."
  },
  {
    "objectID": "tutorials/visualising_distributions.html#bar-plots",
    "href": "tutorials/visualising_distributions.html#bar-plots",
    "title": "Visualising distributions",
    "section": "Bar plots",
    "text": "Bar plots\nThe distribution of categorical variable is almost best visualised using a bar plot.\nBar plots are created in ggplot2 using the geom_bar geometry. If we give this only one aesthetic, it will default to plotting the frequency of each category – that is, how many rows there are with each unique value. For example, using the islay_sites dataset from islay, we could plot the number of sites assigned to each period:\n\nlibrary(ggplot2)\n\nggplot(islay_sites, aes(period)) +\n    geom_bar()\n\n\n\n\nNotice that the default order of the x axis is alphabetical. You’ll usually want to change this to something meaningful: in this case, the periods form a sequence and thus have a natural order. To reorder a categorical axis in ggplot2, you have to transform the variable used to generate it into a factor. To do this we can use the factor() function, specifying the possible levels (periods), in order. We’ll want to use this order every time we plot by period, so we’ll assign the newly created factor back to the period column of our data frame.\n\nperiods &lt;- c(\"Mesolithic\", \"Mesolithic & Later Prehistoric\", \"Later Prehistoric\")\nislay_sites$period &lt;- factor(islay_sites$period, periods)\nggplot(islay_sites, aes(period)) +\n    geom_bar()\n\n\n\n\n\n\n\n\n\n\nFactors\n\n\n\nFactors are how R represents categorical values with a known set of possible values or levels. They are especially useful when the levels are ordered because it provides a way to encode the order into the object itself, which is then automatically picked up by ggplot2 and other packages. The forcats package has a number of useful tools for working with factors, and its cheatsheet is well worth a look.\n\n\nIf what you’re plotting doesn’t have an intrinsic order—the source of information about our sites, for example—a good rule of thumb is to order the bars by size. We can do this easily using fct_infreq() from forcats:\n\nlibrary(forcats)\nggplot(islay_sites, aes(fct_infreq(source))) +\n    geom_bar()\n\n\n\n\nNotice that it automatically places NA values at the end, even though this is the second-largest category.\n\nExercises\n\nWhat can we say about the distribution of sites by period on Islay?\nWhat difference does it make that fct_infreq() inside the ggplot() call in the sources plot, but factor() is outside of it in the periods plot?\n\n\n\nHistograms & density plots\nThe most common way of visualising a numerical variable is a histogram. Visually similar to a bar plot, a histogram sorts each case of the variable into one of a number of equally-spaced bins, and then draws bars corresponding to the frequency of each bin. Histograms are created in ggplot2 using the geom_histogram() geometry which, like geom_bar, only requires one aesthetic. We could use a histogram to visualise the distribution of chipped stone artefacts across the sites on Islay:\n\nggplot(islay_sites, aes(total_chipped_stone)) +\n    geom_histogram()\n\n\n\n\nYou will receive two warnings when generating this plot:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\nWarning: Removed 11 rows containing non-finite values (`stat_bin()`).\nThe second warning means that 11 sites with an NA value in the total_chipped_stone column have been removed from the plot. We can safely ignore this, because it is what we want to happen.\nThe first warning tells us that ggplot has automatically chosen how many bins it should plot (30) and advises us to set a ‘better’ value explicitly. But what is a ‘better’ value? Let’s take a guess and try bin_width = 50 to start with (meaning that the first bin will be 0 to 50, then 50 to 100, etc.):\n\nggplot(islay_sites, aes(total_chipped_stone)) +\n    geom_histogram(binwidth = 50)\n\n\n\n\nThis looks reasonable, but the pattern is rather different from the first plot. Let’s try with a very small and a very large value:\n\nggplot(islay_sites, aes(total_chipped_stone)) +\n    geom_histogram(binwidth = 5)\n\n\n\nggplot(islay_sites, aes(total_chipped_stone)) +\n    geom_histogram(binwidth = 200)\n\n\n\n\nYou can see that the shape of a histogram is highly dependent on the choice of bin width, with different ‘peaks’ and ‘troughs’ appearing and disappearing between the plots. There is no objectively best bin width; you must experiment to find one that is suitable, given your knowledge of the data.\n\n\nExercises\n\nTry using geom_density() instead of geom_histogram(): what is the difference?\nDo density plots have the same problem of bin selection as a histogram?"
  },
  {
    "objectID": "tutorials/visualising_relationships.html",
    "href": "tutorials/visualising_relationships.html",
    "title": "Visualising relationships",
    "section": "",
    "text": "In this tutorial we will continue to learn about visualisation as a tool for exploratory data analysis. We will look at ways of visualising the relationship between two or more variables using bar and column plots, scatterplots, additional aesthetics and facets."
  },
  {
    "objectID": "tutorials/visualising_relationships.html#objectives",
    "href": "tutorials/visualising_relationships.html#objectives",
    "title": "Visualising relationships",
    "section": "Objectives",
    "text": "Objectives\nBy the end of this tutorial you should:\n\nBe able to generate questions about the relationship between two or more variables\nKnow how to produce bar plots, multiple density plots, stacked bar plots, and scatter plots in R using ggplot2\nBe able to refine plots for communication and export them from R"
  },
  {
    "objectID": "tutorials/visualising_relationships.html#prerequisites",
    "href": "tutorials/visualising_relationships.html#prerequisites",
    "title": "Visualising relationships",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nEdward Tufte, The Visual Display Of Quantitative Information (2nd edition), pp. 91–138:\n\nChapter 4, “Data–Ink and Graphical Redesign”\nChapter 5, “Chartjunk: Vibrations, Grids, and Ducks”\nChapter 6, “Data–Ink Maximization and Graphical Design”"
  },
  {
    "objectID": "tutorials/visualising_relationships.html#generating-questions-about-relationships",
    "href": "tutorials/visualising_relationships.html#generating-questions-about-relationships",
    "title": "Visualising relationships",
    "section": "Generating questions about relationships",
    "text": "Generating questions about relationships\nLast week we looked at using visualisation to answer questions about the variation of a variable (its distribution). Although essential for describing and understanding the nature of your dataset, questions about a single variable have a fundamentally limited explanatory value.\nThis week we will start looking at the covariation between two (or more) variables – in plain terms, the relationship between them. With this we can start to gain insights into causality. In statistics, we say that there is a correlation between two variables if one can measurably predict the other. This is not a statement about causality, merely practicality: if you knew two variables were correlated, you could make a good guess about the value of the other.\nThis leads to the well-known adage, “correlation is not causation”. But equally, we should be aware that correlation can be a good hint about causation!\n\nExercises\nGiven a dataset on a burial ground, with the following variables:\n\nsex of the individual\nage of the individual\nage of the burial (i.e. a radiocarbon date)\nnumber of grave goods\nnumber of metal objects amongst the grave goods\n\n\nWhat questions of covariation could we ask of the dataset?\nIf there was a correlation between the age of the individual and the number of grave goods, could that imply causation?\nWhat about a correlation between the number of grave goods and the number of metal objects?"
  },
  {
    "objectID": "tutorials/visualising_relationships.html#visualising-relationships",
    "href": "tutorials/visualising_relationships.html#visualising-relationships",
    "title": "Visualising relationships",
    "section": "Visualising relationships",
    "text": "Visualising relationships\nWork through section 2.5 and 2.6 of *R for Data Science” (2nd ed.)\nYou will then apply these techniques to an archaeological dataset."
  },
  {
    "objectID": "tutorials/visualising_relationships.html#lithic-assemblages-from-islay",
    "href": "tutorials/visualising_relationships.html#lithic-assemblages-from-islay",
    "title": "Visualising relationships",
    "section": "Lithic assemblages from Islay",
    "text": "Lithic assemblages from Islay\nLoad the islay_lithics dataset from islay:\n\nlibrary(islay)\ndata(islay_lithics)\n\nWe can use the head() function to get a quick preview of the data frame:\n\nhead(islay_lithics)\n\n  site_code          region                         period   area flakes blades\n1      LGM1 Loch Gorm South Mesolithic & Later Prehistoric 102450    159     15\n2      LGM2 Loch Gorm South Mesolithic & Later Prehistoric  62497    125      6\n3      LMG4 Loch Gorm South                           &lt;NA&gt;  37480     12      0\n4      LGM5 Loch Gorm South                     Mesolithic  52473    128     18\n5      LGM6 Loch Gorm South              Later Prehistoric  54971     56      4\n6      LGM8 Loch Gorm South                           &lt;NA&gt;  49974     29      1\n  chunks cores pebbles retouched\n1     16    24       0        15\n2     11    20       4        16\n3      1     1       6         3\n4     17    27       7         5\n5      8    18      12        10\n6     20     3       0         5\n\n\nBecause this is an in-built dataset of the package, you can also enter ?islay_lithics to open the help page for the dataset, which contains more information on what it describes.\nAs with the last dataset, it will be useful to turn the period column into a factor now, so that it will automatically be ordered in our subsequent plots:\n\nperiods &lt;- c(\"Mesolithic\", \"Mesolithic & Later Prehistoric\", \"Later Prehistoric\")\nislay_lithics$period &lt;- factor(islay_lithics$period, periods)\n\n\nExercises\n\nGenerate a plot showing the relationship between period and the number of retouched pieces. Is there a correlation? What could explain this?\nTry with two other types of lithics. Does it change your answer?\nGenerate a plot showing the relationship between the number of two types of lithics.\nAdd an aesthetic showing a categorical variable.\nExport the plot."
  },
  {
    "objectID": "tutorials/replication.html",
    "href": "tutorials/replication.html",
    "title": "Replication Workshop",
    "section": "",
    "text": "The purpose of the ‘replication workshop’ is to apply the statistics and R skills you have learned so far to replicating a published finding."
  },
  {
    "objectID": "tutorials/replication.html#objectives",
    "href": "tutorials/replication.html#objectives",
    "title": "Replication Workshop",
    "section": "Objectives",
    "text": "Objectives\nThe replication workshop should enable you to:\n\nIdentify the key findings of a piece of published research in archaeostatistics\nDescribe the data and methods the original authors use to support their findings\nDetermine whether the findings are reproducible and/or replicable"
  },
  {
    "objectID": "tutorials/replication.html#prerequisites",
    "href": "tutorials/replication.html#prerequisites",
    "title": "Replication Workshop",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBocquet-Appel (2011), When the World’s Population Took Off: The Springboard of the Neolithic Demographic Transition, Science 333(6042).\nOne of:\n\nBocquet-Appel (2002), Paleoanthropological Traces of a Neolithic Demographic Transition, Current Anthropology 43(4): 637–650.\nDowney et al. (2014), The Neolithic Demographic Transition in Europe: Correlation with Juvenility Index Supports Interpretation of the Summed Calibrated Radiocarbon Date Probability Distribution (SCDPD) as a Valid Demographic Proxy, PLOS ONE: e105730.\nKohler & Reese (2014), Long and spatially variable Neolithic Demographic Transition in the North American Southwest, Proceedings of the National Academy of Sciences 111(28): 10101–10106."
  },
  {
    "objectID": "tutorials/replication.html#reproducibility-and-replicability",
    "href": "tutorials/replication.html#reproducibility-and-replicability",
    "title": "Replication Workshop",
    "section": "Reproducibility and replicability",
    "text": "Reproducibility and replicability\nReproducibility and replicability are often used interchangeably. Rougly speaking, they ask, given a published research finding, can someone else use the same methods and data to get the same result? The answer to that question is ” According to the journal Rescience C:\n\nReproduction of a computational study means running the same computation on the same input data, and then checking if the results are the same, or at least “close enough” when it comes to numerical approximations. Reproduction can be considered as software testing at the level of a complete study.\nReplication of a scientific study (computational or other) means repeating a published protocol, respecting its spirit and intentions but varying the technical details. For computational work, this would mean using different software, running a simulation from different initial conditions, etc. The idea is to change something that everyone believes shouldn’t matter, and see if the scientific conclusions are affected or not.\nReproduction verifies that a computation was recorded with enough detail that it can be analyzed later or by someone else. Replication explores which details matter for reaching a specific scientific conclusion. A replication attempt is most useful if reproducibility has already been verified. Otherwise, if replication fails, leads to different conclusions, you cannot trace back the differences in the results to the underlying code and data."
  },
  {
    "objectID": "tutorials/replication.html#exercise",
    "href": "tutorials/replication.html#exercise",
    "title": "Replication Workshop",
    "section": "Exercise",
    "text": "Exercise\nIn groups, take one of the papers above and:\n\nIdentify at least one key finding of the paper to replicate (e.g. a specific figure)\nIdentify the a) data, b) data manipulation and c) analyses the authors use to support the finding\nReproduce the finding by repeating the analysis with the original data, as closely as possible\nIdentify at one or more ‘technical details’ that could (but hopefully don’t!) affect the result\nReplicate the finding by repeating the analysis with alterations to these data"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nTutorial\nPrerequisites\n\n\n\n\n09\nIntroduction\n—\n\n\n\nExploring archaeological data\n\n\n\n10\nExploring data with R\nEvergreen notesInstall R on your computer\n\n\n11\nVisualising distributions\n The Role of Visualization in Exploratory Data Analysis\n\n\n12\nVisualising relationships\n The Visual Display of Quantitative Information, ch. 4, 5 & 6\n\n\n13\nSummary statistics\n Quantifying Archaeology, ch. 4\n\n\n\nWorking with archaeological data\n\n\n\n14\nImporting and transforming data\n Data Organization in Spreadsheets\n\n\n15\nSpring vacation\n—\n\n\n16\nData analysis workflows\n Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation\n\n\n17\nReplication Workshop I\n When the World’s Population Took Off One paper to be replicated\n\n\n18\nReplication Workshop II\n—\n\n\n\nModelling archaeological data\n\n\n\n19\nHypothesis testing\n Quantifying Archaeology, ch. 5 & ch. 6\n\n\n20\nRegression\n Quantifying Archaeology, ch. 8 & ch. 9\n\n\n21*\nMultivariate analysis\n—\n\n\n22\nReview & course evaluation\n—\n\n\n\n* Substitute instructor"
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography & Further Reading",
    "section": "",
    "text": "Shennan, S. 1997. Quantifying Archaeology. Second edition. Available online at https://doi.org/10.1515/9781474472555\nDrennan, R. D. Statistics for Archaeologists: A Commonsense Approach. Second edition."
  },
  {
    "objectID": "bibliography.html#archaeostatistics-textbooks",
    "href": "bibliography.html#archaeostatistics-textbooks",
    "title": "Bibliography & Further Reading",
    "section": "",
    "text": "Shennan, S. 1997. Quantifying Archaeology. Second edition. Available online at https://doi.org/10.1515/9781474472555\nDrennan, R. D. Statistics for Archaeologists: A Commonsense Approach. Second edition."
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "Information for students",
    "section": "",
    "text": "We are dedicated to providing a welcoming and supportive learning environment for all students, regardless of their background, identity, physical appearance, or manner of communication. Any form of language or behavior used to exclude, intimidate, or cause discomfort will not be tolerated. This applies to all course participants (instructor, students, guests). In order to foster a positive and professional learning environment, we ask the following:\n\nPlease let us know if you have a name or set of preferred pronouns that you would like us to use\nPlease let us know if anyone in class says something that makes you feel uncomfortable\n\nIf an instructor should be the one to say or do something that makes a student uncomfortable, you should feel free to contact the Director of the Prehistoric Archaeology of Bern University.\n\nIf you believe you have been a victim of any form of discrimination, you have the right to report it to the University.\n\nIn addition, we encourage the following kinds of behaviors:\n\nUse welcoming and inclusive language\nShow courtesy and respect towards others\nAcknowledge different viewpoints and experiences\nGracefully accept constructive criticism\n\nAlthough we strive to create and use inclusive materials in this course, there may be overt or covert biases in the course material due to the lens with which it was written. Your suggestions about how to improve the value of diversity in this course are encouraged and appreciated.\nPlease note: The University of Bern is committed to a non-discriminatory environment. This applies in particular to discrimination on the basis of gender (sexism) and sexual assaults (sexual harassment): we are all entitled to protection of our personal integrity at work and during our studies and to the respect of our limits. The University of Bern is also legally obliged to protect its members from sexual harassment. In particular, superiors are also responsible for looking out and reacting."
  },
  {
    "objectID": "info.html#classroom-conduct",
    "href": "info.html#classroom-conduct",
    "title": "Information for students",
    "section": "",
    "text": "We are dedicated to providing a welcoming and supportive learning environment for all students, regardless of their background, identity, physical appearance, or manner of communication. Any form of language or behavior used to exclude, intimidate, or cause discomfort will not be tolerated. This applies to all course participants (instructor, students, guests). In order to foster a positive and professional learning environment, we ask the following:\n\nPlease let us know if you have a name or set of preferred pronouns that you would like us to use\nPlease let us know if anyone in class says something that makes you feel uncomfortable\n\nIf an instructor should be the one to say or do something that makes a student uncomfortable, you should feel free to contact the Director of the Prehistoric Archaeology of Bern University.\n\nIf you believe you have been a victim of any form of discrimination, you have the right to report it to the University.\n\nIn addition, we encourage the following kinds of behaviors:\n\nUse welcoming and inclusive language\nShow courtesy and respect towards others\nAcknowledge different viewpoints and experiences\nGracefully accept constructive criticism\n\nAlthough we strive to create and use inclusive materials in this course, there may be overt or covert biases in the course material due to the lens with which it was written. Your suggestions about how to improve the value of diversity in this course are encouraged and appreciated.\nPlease note: The University of Bern is committed to a non-discriminatory environment. This applies in particular to discrimination on the basis of gender (sexism) and sexual assaults (sexual harassment): we are all entitled to protection of our personal integrity at work and during our studies and to the respect of our limits. The University of Bern is also legally obliged to protect its members from sexual harassment. In particular, superiors are also responsible for looking out and reacting."
  },
  {
    "objectID": "info.html#access-accommodations",
    "href": "info.html#access-accommodations",
    "title": "Information for students",
    "section": "Access & accommodations",
    "text": "Access & accommodations\nAll students deserve access to the full range of learning experiences, and we are committed to creating inclusive and accessible learning environments consistent with federal and state laws. If you feel like your performance in class is being impacted by your experiences outside of class, please talk with us.\n\nDisabilities\nIf you have already established accommodations with Disability Resources for Students (DRS), please communicate your approved accommodations to us at your earliest convenience so we can discuss your needs in this course. If you have not yet established services through DRS, but have a temporary health condition or permanent disability that requires accommodations (e.g., mental health, learning, vision, hearing, physical impacts), you are welcome to contact DRS at 206-543-8924 or via email or their website. DRS offers resources and coordinates reasonable accommodations for students with disabilities and/or temporary health conditions. Reasonable accommodations are established through an interactive process between you, your instructor(s) and DRS.\n\n\nReligious observances\nStudents who expect to miss class or assignments as a consequence of their religious observance will be provided with a reasonable accommodation to fulfill their academic responsibilities. Absence from class for religious reasons does not relieve students from responsibility for the course work required during the period of absence. It is the responsibility of the student to provide the instructor with advance notice of the dates of religious holidays on which they will be absent. Students who are absent will be offered an opportunity to make up the work, without penalty, within a reasonable time, as long as the student has made prior arrangements."
  },
  {
    "objectID": "info.html#academic-integrity",
    "href": "info.html#academic-integrity",
    "title": "Information for students",
    "section": "Academic integrity",
    "text": "Academic integrity\nFaculty and students at the University of Bern are expected to maintain the highest standards of academic conduct, professional honesty, and personal integrity. Plagiarism, cheating, and other academic misconduct are serious violations of the Scientific Code of Conduct. Students who have been guilty of a violation will receive zero points for the assignment in question."
  },
  {
    "objectID": "info.html#mental-health",
    "href": "info.html#mental-health",
    "title": "Information for students",
    "section": "Mental health",
    "text": "Mental health\nWe are in the midst of an historic pandemic that is creating a variety of challenges for everyone. If you should feel like you need some help, please consider the following resources available to students.\nIf you are experiencing a life-threatening emergency, please dial 112.\nCounseling Center\nPhone: 031 / 635 24 35"
  }
]